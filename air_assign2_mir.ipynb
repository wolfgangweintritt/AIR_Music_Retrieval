{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>188.980 Advanced Information Retrieval</h1>\n",
    "<h2>Assignment 2 - Music IR</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "**Group Number:**\n",
    "\n",
    "* **Group members:**\n",
    " * **Full Name:**\n",
    "   * *Matrikelnummer:*\n",
    "   * *Studienkennzahl:*\n",
    " * **Full Name:**\n",
    "   * *Matrikelnummer:*\n",
    "   * *Studienkennzahl:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The aim of this exercise is to get insights into music information retrieval practices. You will follow a content based approach which is based on the idea that the semantically relevant information is provided in the audio itself. Thus the aim is to identify and extract this information in a meaningful representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:32:43.591000",
     "start_time": "2017-05-15T23:32:43.496000"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# plotting functions\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os              # access to operating system functions\n",
    "import glob            # reg-ex based file-iterator\n",
    "\n",
    "# scientific computing\n",
    "import numpy as np     # numerical computing\n",
    "import pandas as pd    # powerful data processing library\n",
    "\n",
    "# audio feature extraction\n",
    "import librosa\n",
    "from rp_extract import rp_extract\n",
    "\n",
    "# machine learning\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from utility_functions import show_classification_results, plot_confusion_matrix, show_query_results\n",
    "\n",
    "# misc\n",
    "import itertools\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your data\n",
    "\n",
    "First, please download the specified data-set, extract it locally to your hardisk and specify the full-path to the extracted directory to the *AUDIO_COLLECTION_PATH* variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:13:48.264000",
     "start_time": "2017-05-15T23:13:48.186000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put 'genres.tar.gz' in 'collection/' and 'tar -xf' it there\n",
    "AUDIO_COLLECTION_PATH = \"collection/genres\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare file-lists and ground-truth labels**\n",
    "\n",
    "Define the collection of files to process and the corresponding class labels which is also known as *ground truth*.\n",
    "\n",
    "* create a list of filenames and a list of corresponding labels\n",
    "  * e.g. use [glob](https://docs.python.org/2/library/glob.html) to iterate over the class directories in the music collection\n",
    "  * use the names of the subdirectories as label names\n",
    "  * iterate through all class directories and get all filenames with the extension '.mp3'\n",
    "  * store the filename and its corresponding label in the two lists *filenames* and *labels*\n",
    "  \n",
    "Take care that both lists are synchronously aligned and that each label corresponds to the right filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:13:48.642000",
     "start_time": "2017-05-15T23:13:48.568000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill these lists with values\n",
    "filenames = []\n",
    "labels    = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:13:49.505000",
     "start_time": "2017-05-15T23:13:49.408000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add your code here\n",
    "for genre in os.listdir(AUDIO_COLLECTION_PATH):\n",
    "    genre_dir = \"%s/%s\" % (AUDIO_COLLECTION_PATH, genre)\n",
    "    for filename in os.listdir(genre_dir):\n",
    "        full_filename = \"%s/%s\" % (genre_dir, filename)\n",
    "        # every filename and its corresponding genre share the same index\n",
    "        filenames.append(full_filename)\n",
    "        labels.append(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Feature Extraction\n",
    "\n",
    "*Feature extraction* is a crucial part of content-based approaches. The goal is to transform and reduce the information provided by the digital audio signal into a set of semantically descriptive numbers. A typical CD quality mainstream radio track has an average length of three minutes. This means, that song is digitally described in Pulse-code Modulation (PCM) by 15.9 million numbers (3 [minutes] x 60 [seconds] x 2 [stereo channels] x 44100 [sampling rate]). Using CD-quality 16bit encoding this information requires 30.3MB of memory. Besides music specific reasons, the computational obstacles concerned with processing of huge music collections make raw audio processing a suboptimal solution. Feature design and implementation tries to overcome technological obstacles of digital audio and to extract essential music properties that can be used to analyze, compare or classify music. \n",
    "\n",
    "You will use the common music features are descriptors for timbre \\cite{logan2000mel}, rhythm \\cite{lidy10_ethnic} or general spectral properties \\cite{tzanetakis2000marsyas,lartillot2007matlab}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:13:50.663000",
     "start_time": "2017-05-15T23:13:50.589000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill these lists with values\n",
    "audio_features = {}\n",
    "audio_features[\"mfcc\"]   = []\n",
    "audio_features[\"chroma\"] = []\n",
    "audio_features[\"ssd\"]    = []\n",
    "audio_features[\"rp\"]     = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* iterate over all audio files\n",
    "    1. load audio data using the [librosa.core.load](http://librosa.github.io/librosa/generated/librosa.core.load.html) function. The feature extractors you will use expect a mono signal as input, so take care to appropriately load or convert the audio data.<br><br>\n",
    "    2. extract music features<br><br>\n",
    "        2.1. [Mel-frequency cepstral coefficients (MFCC)](http://librosa.github.io/librosa/generated/librosa.feature.mfcc.html) (librosa): extract 12 coefficients using a FFT-window size of 1024 samples with 50% overlap (this corresponds to a hop-length of 512 samples). To correctly calculate the coefficients the samplerate of the audio file has to be provided.<br>\n",
    "         2.2. [Pitch-classes / Chroma](http://librosa.github.io/librosa/generated/librosa.feature.chroma_stft.html) (librosa): extract the chroma features using a FFT-window size of 1024 samples with 50% overlap, similar to the MFCCs. Also supply the correct samplerate.<br>\n",
    "         2.3. The features extracted using the libros-library are provided on a frame-level. They are calculated for each FFT-window and thus the results are lists of feature vectors. The machine learning algorithms you will be using expect a single-vector per instance. Thus, the extracted feature vectors need to be aggregated into a single-vector representation. You will be using the simple approach of just calculating their mean and standard-deviations. Perform the following operations for the *MFCC* and *chroma* features:\n",
    "      * use the numpy functions *mean* and *std* to calculate the statistical moments of the extracted features along the appropriate axis.\n",
    "      * use the numpy function *concatenate* to concatenate the statistical moments into a single vector\n",
    "  \n",
    "    2.4. *Statistical Spectrum Descriptors* and *Rhythm Patterns*: use the following code to extract the psychoaccoustic feature-sets. The function returns a dictionary of extracted features. The Statistical Spectrum Descriptors are stored using the key 'ssd' and the Rhythm Patterns using the key 'rp'. The features are already single vectors so no post-processing is required.\n",
    "\n",
    "                rp_features = rp_extract(wave_data, \n",
    "                                         samplerate, \n",
    "                                         extract_ssd = True, \n",
    "                                         extract_rp  = True,\n",
    "                                         skip_leadin_fadeout = 0, \n",
    "                                         verbose     = False)\n",
    "\n",
    "    3. append the extracted features to the dictionary embedded lists which are provided above.<br><br>\n",
    "    4. finally iterate over the dictionary keys and convert the lists of extracted feature-vectors into feature-spaces by converting them into numpy arrays using the numpy *asarray* function.\n",
    "\n",
    "  \n",
    "The feature extraction will take about 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T22:57:30.406000",
     "start_time": "2017-05-15T22:57:30.398000"
    }
   },
   "outputs": [],
   "source": [
    "# progressbar visualization widget to estimate processing time\n",
    "pbar = progressbar.ProgressBar()\n",
    "\n",
    "# iterate over all files of the collection\n",
    "for audio_filename in pbar(filenames):\n",
    "    \n",
    "    # 1. load audio\n",
    "    \n",
    "    # 2. extract features\n",
    "    # - 2.1. mfcc\n",
    "    # - 2.2. chroma\n",
    "    # -- 2.3. aggregate frame based mfcc and chroma vectors into single feature vectors\n",
    "    # - 2.4. ssd, rp\n",
    "    \n",
    "    # 3. append to provided lists\n",
    "    \n",
    "# 4. convert lists of vectors to numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Classification\n",
    "\n",
    "Music Genre Recognition is a well researched Music Information Retrieval task. As for many other audio-based MIR tasks the algorithmic design consists of two parts. First, audio-content descriptors are extracted from the audio signal. In a second step these features are used to train machine learning based models, using popular supervised classifiers including k-nearest neighbors (k-NN), Gaussian mixture models (GMM) or Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-preprocessing\n",
    "\n",
    "The ground-truth labels are provided using their class names, in our case they are names of music genres, and thus are string values. The applied machine-learning algorithms expect numerical class labels. Thus, you are required to pre-process your data:\n",
    "\n",
    " * use sklearn's *LabelEncoder* to convert string- into numeric-labels\n",
    " * fit the encoder on the provided genre labels\n",
    " * create a list called *encoded_labels* using the *transform* method of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:40:57.709000",
     "start_time": "2017-05-15T23:40:57.706000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Experiments\n",
    "\n",
    "Iterate over all feature-sets. For each previously extracted feature perform the following steps:\n",
    "\n",
    "1. A common way to evaluate the performance of a machine learning approach is to perform a cross-validation. For this exercise you will be using a stratified K-Fold cross-validation. A K-Fold cross-validation splits the data-set into K equally-sized parts and uses K-1 parts to train the classifier and the remaining part to test it. The stratification asserts that the frequency distribution of the class labels within the folds corresponds to the original distribution of the ground-truth assignment. As a first step create a shuffled *StratifiedKFold* cross-validation object which generated 10 *splits* (10-fold cross-validation).<br><br>\n",
    "\n",
    "2. For each train/test-split apply the following commands:\n",
    "\n",
    "    2.1. Pre-process the feature-space: audio features are usually abstract content descriptors and have varying value ranges. Many machine learning algorithms expect all features to have the same value ranges. Thus, varying ranges will have a bias towards features with high value ranges. You will use Zero-Mean Unit-Variance Normalization - also referred to as Z-Score normalization or standardization, which subtracts the mean value from each feature and divides it by its variance. An important fact to note here, is that the parameters of the scaler are part of the trained classification model. They have to be assessed from the training-split, because the test-split is supposed to be unseen data. Use the sklearn *StandardScaler* and process the feature-data similar to the *LabelEncoder* by fitting the scaler using only the training-instances. After fitting the scaler transform the feature vectors of the training-split (but do not overwrite the original features!).<br>\n",
    "    2.2. You will use the Support Vector Machine (SVM) classifier to train a model for automatic genre prediction. The sklearn recently made some efforts to standardize their APIs. Thus, also the SVM classifier adheres to the same pattern. First, create a *LinearSVC* classifier using its default parameters. Then, train the SVM using the *fit* method by supplying the train-split of the feature vecotrs and the ground-truth labels.<br>\n",
    "    2.3. similar to 2.1. standardize the test-split using the already fitted scaler.<br>\n",
    "    2.4. use the fitted SVM to predict the scaled test-split feature vectors using its *predict* method.<br>\n",
    "    2.5. for each fold, store the true and predicted numeric labels.<br><br>\n",
    "    \n",
    "3. for each feature-set, store the true and predicted labels of the evaluation back to the provided dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill this dictionary with values\n",
    "classification_results = {}\n",
    "\n",
    "pbar = progressbar.ProgressBar()\n",
    "\n",
    "for feature_name in pbar(audio_features.keys()):\n",
    "    \n",
    "    # 1. crossvalidation\n",
    "    \n",
    "    # 2. run cross-validation\n",
    "        \n",
    "        # --- TRAIN ---\n",
    "        \n",
    "        # 2.1. fit scaler\n",
    "        # scale training-partition of features\n",
    "        \n",
    "        # 2.2. create classifier\n",
    "    \n",
    "        # fit classifier with scaled training-set\n",
    "        \n",
    "        # --- TEST ---\n",
    "        \n",
    "        # 2.3. scale test-partition of features\n",
    "        \n",
    "        # 2.4. use fitted classifier to predict labels of test-partition\n",
    "        \n",
    "        # 2.5. store the true and predicted labels\n",
    "        \n",
    "    # 3. store the classification results back to the dictionary\n",
    "    classification_results[feature_name] = [y_true, y_pred]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the classification result table**\n",
    "\n",
    "If you have supplied the results in the requested format, you should be able to use our provided function to plot the classification results-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:32:51.486000",
     "start_time": "2017-05-15T23:32:51.386000"
    }
   },
   "outputs": [],
   "source": [
    "show_classification_results(classification_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-set combinations\n",
    "\n",
    "Music is a complex concept which cannot be described easily by a single property. Music is not just rhythm or a certain timbre/sound. It is more a mixture of all of it. This also applies to music features. Different combinations of features perform better than others, but there is no rule of thumbs which ones. This has to be evaluated according the underlying dataset, the applied classifier, the normalization method, etc. All these parameters affect the performance of the classification model.\n",
    "\n",
    "For this exercise, extend the previously developed classification approach by a feature combination component. Because the dataset is small, it is feasable to evaluate all possible feature-set combinations. Execute to the following cell to calculate all combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:33:19.391000",
     "start_time": "2017-05-15T23:33:19.316000"
    }
   },
   "outputs": [],
   "source": [
    "combinations = []\n",
    "\n",
    "for i in range(1,len(audio_features.keys())):\n",
    "    combinations.extend(itertools.combinations(audio_features.keys(), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining features is also referred to as fusion and two common approaches, early fusion and late fusion, exist. You will be using an early fusion approach to extend the classification experiment. To do so, either copy and paste the code of the evaluation-loop into a new cell, or adapt the code of the existing cell:\n",
    "\n",
    "* in the for loop, change the iterable from *audio_features.keys()* to *combinations*. You are also adviced to change the variable name *feature_name* to reflect the circumstance that the variable contains now a list of feature names.\n",
    "* implement the early fusion approach. This is accomplished by appending one feature vector to the other to create a combined feature vector. This can be solved by combining the feature-spaces. Use the numpy function *concatenate* to concatenate the feature combinations along feature axis. This can be done before the cross-validation loop or within. \n",
    "* be sure to reset the *classification_results* dictionary before running the evaluation again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the classification result table and confusion matrix**\n",
    "\n",
    "If you have supplied the results in the requested format, you should be able to use our provided function to plot the classification results-table and the confusion matrix. A confusion matrix is a convenient tool to assess class-level model performance. It shows which classes are mutually confused. This can help to tune the parameters of the model or to choose different features/combinations.\n",
    "\n",
    "To estimate the performance of the evaluated models:\n",
    "* plot four result tables and confusion matrices including the top-performing result.\n",
    "* write a paragraph where you summarize the conclusions of the classification experiments\n",
    "  * which feature-set/feature-set-combinations perform best?\n",
    "  * for which classes are they performing best?\n",
    "  * which classes are confused most?\n",
    "  * what does this tell about the audio features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:33:34.990000",
     "start_time": "2017-05-15T23:33:34.849000"
    }
   },
   "outputs": [],
   "source": [
    "show_classification_results(classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:33:52.202000",
     "start_time": "2017-05-15T23:33:51.577000"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(classification_results[\"ssd\"], l_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Write your conclusions here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Retrieval / Query by example\n",
    "\n",
    "The second part of the assignment focuses on retrieving similar songs to a given example. Content based music similarity can also be defined upon the numeric feature-space. Thus, you will use the extracted music features from the classification experiments, but now you will implement a late fusion approach. The problem with early fusion is that results will skew towards features-sets with higher dimensionality. Late fusion represents a convenient method to overcome this problem by combining the result-spaces instead of the feature-spaces. For each feature first the similarities are calculated separately and then aggregated into a final estimation. Thus, every feature has an equal influence on the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Just like for the classification experiments the feature-spaces first need to be normalized. \n",
    "\n",
    "* iterate over all feature-sets\n",
    "* use the *StandardScaler* as above, but because there are no train/test-splits, fit the scaler using all instances of a feature-set, and finally perform the transformation\n",
    "* you can overwrite the extracted *audio_features*; their original values are not required further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:34:07.262000",
     "start_time": "2017-05-15T23:34:07.153000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement a query-by-example system:**\n",
    "\n",
    "* define a *query_id* variable. This represents the index into the feature-space. The feature-vector with the corresponding id will be used to find similar songs.\n",
    "* define a list of feature-set names. This is equivalent to the feature combinations you have implemented for the classification experiments.\n",
    "* define a corresponding list of feature-weights. Usually weights scale from 0.0 to 1.0 and represent the influence of a feature-set on the final result.\n",
    "* for each feature-set of the specified feature-set combination:\n",
    "  * calculate the Eucledian-distance from the query-feature-vector to every other vector of a feature-set:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\sum (audio\\_features[feature\\_name] - audio\\_features[feature\\_name][query\\_id])^{2}}\n",
    "$$\n",
    "\n",
    "  * this will result in a distance value for each feature-vector representing the dis-similarity to the query-vector.\n",
    "  * due to variances in dimensionality and value-ranges these distances vary between the feature-sets. To normalize the impact of a distinct feature-set in a late-fusion approach, you have to normalize each resulting distance-list by dividing it by its maximum value.\n",
    "  * multiply each normalized distance-list with its corresponding feature-weight\n",
    "  * calculate the sum of the weighted distance-lists\n",
    "  * finally, use apply numpy function *argsort* on the final results to get the indexes to the feature-vectors ranked by their ascending summed distances. Thus, the top results have the lowest distance/dis-similarity to the query-vector and are expected to sound similar to the query-song.\n",
    "  * use our provided method to display the retrieval results:\n",
    "  \n",
    "        show_query_results(filenames, labels, ranked_index)\n",
    "      \n",
    "The displayed result-table contains HTML5 audio player tags. Unfortunately, most Internet browsers have a security-lock to prevent Web-pages from referencing or accessing local files. \n",
    "\n",
    "* To enable the playback of the audio-files, open a command-prompt and change to the root directory of the provided dataset (*AUDIO_COLLECTION_PATH*). \n",
    "* start the Python simple web-server on port 5555. This will create a simple web-server which handles requests to the local files:\n",
    "\n",
    "        > python -m SimpleHTTPServer 5555\n",
    "\n",
    "Alternatively, you can resort to one of the following [http static server one-liners](https://gist.github.com/willurd/5720255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T23:42:08.064000",
     "start_time": "2017-05-15T23:42:08.062000"
    }
   },
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiment with different feature-set combinations and feature-weights using different query-songs.\n",
    "* show two good and two bad performing examples using the *show_query_results* function\n",
    "* write a paragraph where you summarize the conclusions of the query-by-example experiments\n",
    "  * which feature-sets are performing best and why?\n",
    "  * listening to the results, what is the perceived quality?\n",
    "    * in which terms do the top-ranked results sound similar\n",
    "    * are there songs that do not fit at all?\n",
    "    * do they at least share some acoustic similarity with the query-song (e.g. rhythm, instrumentation, pitch, etc.)?\n",
    "  * compare the query-by-example precision with the classification confusion-matrix for the corresponding feature-set-combination. Are the top confusions also predominant mismatches within the top-ranked retrieval results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Write your conclusions here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_section_display": "none",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
